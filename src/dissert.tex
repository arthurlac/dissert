\documentclass[a4paper,10pt]{article}
\usepackage{listings}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{parskip}
\usepackage{stackrel}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]
\newtheorem{theorem}{Thereom}[section]

\usepackage{tikz-cd}
\usetikzlibrary{arrows}
\tikzset{
    commutative diagrams/.cd,
    arrow style=tikz,
    diagrams={>=space}
}
\tikzcdset{
    arrow style=tikz,
    diagrams={>={Straight Barb[scale=0.8]}}
}

\setlength{\parindent}{0pt}
\setlength{\parskip}{1em}

\title{A comparison of algebraic effects and monads}

\begin{document}
\section{Abstract}

Effects are tricky, and essential!
Programmers have long sought to make them more predictable;
this may come at the cost of complexity.
In this paper I will present and explore
the relation and use of monads and algebraic effects,
I will also try to establish the theoretical foundations of these concepts.
I will present example code showing
Explain where monad-like patterns arise in imperative programming and how monads are often a better alternative

THE KEY POINT IS MONADS ARE GREAT FOR STRUCTURING PROGRAMS TO BE ~good

elucidate the relation between monads and algebraic effects
explain the use of both

The goal of this paper will be to explain both monads and algebraic effects
and their effectiveness as programming techniques.
At the end of this paper will be an abridged version which will
As such the success can be judged by how well it explains the use and benefits of both monads and algebraic effects.
Demonstrate monad-like programming techniques are already common in imperative languages

An overview of how is paper is structured is as such

Monads and Computation
Notions of Computation Determine Monads
Algebraic Effects
Rewriting our examples
Purely Functional Data structures
Why Functional Programming Matters
Why functional Progamming matters
Conclusion: category theory times functional programming matters
Category theory as a framework for better software

Finally I will present an abridged version which forgoes some theory and background
with the goal of introducing and advocating for the concepts
target audience being programmers

\pagebreak
\tableofcontents
\pagebreak

\section{What are effects?}
Firstly, if we are going to assert effects are essential,
we should define what an effect exactly is.

\begin{example}
    We can give several categories !!!! of computational effects
    which illustrate
    \begin{itemize}
        \item Input/Output
        \item Mutable state
        \item Exceptions
        \item Non-Determinism and Probabilistic Non-Determinism
        \item Continuations
    \end{itemize}
\end{example}

Note that effects $\neq$ side effects,
for example input/output and mutable states are side effects
however non-determinism is not.
"a side effect if it modifies some state outside its scope or has an observable interaction with its calling functions or the outside world besides returning a value."CHANGEME
\cite{hughes1989functional}
\begin{definition}
    An \textit{effect} is
\end{definition}

\begin{definition}
    A \textit{side effect} is
\end{definition}

\subsection{Referential transparency}
\begin{definition}
    An \textit{referential transparency} is
\end{definition}
WIKI REMOVE ME
An expression is said to be referentially transparent if it can be replaced with its corresponding value without changing the program's behavior.[1] As a result, evaluating a referentially transparent function gives the same value for same arguments. Such functions are called pure functions. An expression that is not referentially transparent is called referentially opaque.
\begin{definition}
    An \textit{referential opacity} is
\end{definition}

What does it mean to be pure?
\begin{definition}
    \textit{Purity} means
\end{definition}

Relation to lambda calculus and $\beta$-equality
In a pure language we have $\beta$-equality

CONC WE NEED EFFECTS BUT THEY ARE TRICKY, SO VERY TRICKY
We have to ask in what ways can we deal with effects?
What tools do we have as a programmer at our disposal here????
Following this we will present two ideas both originating from CT
which give us tools for effectful programming

\section{Categorical Views of Effects}
\subsection{Overview}
%Category theorists invented monads in the 1960s to concisely of universal algebra􏰊\cite{wadler1990comprehending}
Originally Haskell \cite{} adopted monads due to lazy evaluation
From Moggi’s assignment to each set X of the set TX of values as- sociated with a computational effect, one has passed to the study of the operations associated with the computational effect
\cite{hyland2007category}
Following Moggi we have \cite{plotkin2001adequacy}
The Lawvere theory LE for exceptions is the free Lawvere theory generated by E operations raise : 0  1,
where E is a set of exceptions.
In terms of operations and equations,
this corresponds to an E-indexed family of nullary operations with no equations.
%The monad on Set generated by LE is TE = − + E. More generally, the forgetful functor UL : Mod(LE,C) C induces the monad
%−+E on C, where E is the E-fold copower of 1, i.e.,  exists in C.
It is shown in [39]
that this countable Lawvere theory induces Moggi’s side-effects monad $(S \times \textrm{-})^S$ on
Set.
\cite{hyland2007category}

Wadler \cite{wadler1990comprehending} demonstrated the use of monads in a pure
functional programming language to simulate effects.

OPERATIONS GENERATE MONADS \cite{plotkin2001adequacy}
Plotkin and Power verified that and, later joined by Hyland, saw that what were then regarded as com- putational effects could, with one exception, fruitfully be seen as an instance and a development of universal algebra: interactive input/output was soon recognised as an example involving no equations; it was seen how to incorporate local state naturally; and the various ways of combining computational effects proved to be simple instances of combining Lawvere theories.


\subsection{Categories}
First we must introduce the concept of \textit{categories},

\begin{definition}
    A \textit{category} $C$ consists of
    \begin{itemize}
        \item A set $Ob\,C$, elements of which are called \textit{objects} of $C$.
        \item For each $X, Y \in Ob\,C$
            a set $Hom(X,Y)$ called the \textit{homset} from $X$ to $Y$.
        \item A \textit{composition function} $\circ$ such that
            \begin{equation}
                \circ : Hom(X,Y) \times Hom(Y,Z) \rightarrow Hom(X,Z)
            \end{equation}
        \item For all $X \in Ob\,C$ an element $Id_X$ of $Hom(X,\ X)$
            such that $f \circ Id_X = Id_Y \circ f = f$
        \item Composition is associative.  $f \circ (g \circ h) = (f \circ g) \circ h$
    \end{itemize}
\end{definition}

\par
An element $f$ of $Hom(X,Y)$ is called an \textit{arrow},
or a \textit{morphism}. The object $X$ is called the \textit{domain} of $f$ and $Y$ is
the \textit{codomain}.\\

\begin{example}
    The immediate example of a category is the category of sets.
    The objects in $C$ are (small) sets,
    a morphism from $X$ to $Y$ is a function $f : X \rightarrow Y$.
    The composition of Set is given by composition of functions,
    and the identity maps are given by the identity functions.
\end{example}

\subsection{Functors}
\begin{definition}
    A \textit{functor} $U : C \rightarrow D$ consists of
    \begin{itemize}
        \item A function $Ob\,U : Ob\,C \rightarrow Ob\,D$.
        \item A function $U : Hom_C(X,Y) \rightarrow Hom_D(UX, UY)$
            such that $U$ respects both composition and identity.
            I.e.
            \begin{equation}
                Uf \circ Ug = U(f \circ g) \quad U\,Id_X = Id_{UX}
            \end{equation}
    \end{itemize}
\end{definition}

\begin{definition}
    An \textit{endofunctor} is a functor $U : C \rightarrow C$;
    i.e. the domain and codomain of the functor are the same category $C$.
\end{definition}

\subsection{Natural transformations}
\begin{definition}
    Given categories $C$ and $D$,
    with functors $U, V : C \rightarrow D$
    a \textit{natural transformation} $\alpha : U \rightarrow V$
    consists of
    \begin{equation}
        \forall\ X \in Ob\,C\ \textrm{a map} \ \alpha_X : UX \rightarrow VX
    \end{equation}
    such that $\forall\ f : X \rightarrow Y$ the following commutes
    \begin{center}
        \begin{tikzcd}[sep=large]
            UX \rar{\alpha_X} \dar[swap]{Uf} & VX \dar{Vf} \\
            UY \rar{\alpha_Y}                & VY
        \end{tikzcd}
    \end{center}
    A natural transformation can be considered a morphism of functors.
\end{definition}

\subsection{Monads}
\begin{definition}
    A \textit{monad} is defined as the triple
    \begin{itemize}
        \item An endofunctor $T : C \rightarrow C$
        \item A natural transformation $\eta : 1_{C} \rightarrow T$
        \item A natural transformation $\mu : T^2 \rightarrow T$
    \end{itemize}
    Such that the following diagrams commute
    \begin{center}
        \begin{tikzcd}[sep=large]
            T \rar{\eta_T} \drar[swap]{1_{C}} & T^2 \dar{\mu} & \lar[swap]{T\eta} \dlar{1_{C}} T \\
                                               & T            &
        \end{tikzcd}
        \quad
        \begin{tikzcd}[sep=large]
            T^3 \rar{T\mu} \dar[swap]{\mu_T} & T^2 \dar{\mu} \\
            T^2 \rar[swap]{\mu}                    & T
        \end{tikzcd}
    \end{center}
\end{definition}

\subsection{Kleisli Category}
\begin{definition}
    Given a monad $(T,\eta,\mu)$ over a category $C$,
    the \textit{Kleisli category} $C_T$ consists of
    \begin{itemize}
        \item Objects of $C_T$ are objects from the underlying category $C$.
        \item $Hom_{C_T}(X,Y) = Hom_C (X,TY)$
        \item Identity morphisms in $C_T$ are $\eta$ in $C$
        \item Composition $f \circ g$ is $\mu(Tf)g$
    \end{itemize}
\end{definition}

Composition in $C_T$ can be described in more detail via the operator
\begin{equation}
    (-)^{*} : Hom(X, TY) \rightarrow Hom(TX, TY)
\end{equation}
where given a morphism $f: X \rightarrow TY$ we have
\begin{equation}
    f^{*} = \mu_{Y} \circ Tf
\end{equation}
Note that
\begin{equation}
    \eta_{X}^{*} = Id_{TX}
    \quad\textrm{and}\quad
    f^{*} \circ \eta _{X} = f
\end{equation}
Then we can define the \textit{Kleisli operator} $\gg$ where
\begin{equation}
    g \gg f = g^{*} \circ f
\end{equation}
\begin{equation}
    x
    \stackrel{f}{\rightarrow}     T y
    \stackrel{T g}{\rightarrow}   T T z
    \stackrel{\mu z}{\rightarrow} T z
\end{equation}
where $\gg$ has these axioms
\begin{equation}
    (f \gg g) \gg h \equiv f \gg (g \gg h)
\end{equation}
\begin{equation}
    \eta_Y \gg f \equiv f \equiv f \gg \eta_X
\end{equation}

\subsection{$\lambda_c$-Calculus}
Moggi \cite{moggi1989computational} introduced categorical semantics of computation
based on monads.
He defines a computational model which allows computations with effects such as non-deterministic 
computation, side effects, and continuations.

Cartesian closed category to model the simply typed lambda calculus
Moggi
%T() is the functor ( ×S)S, where S is a nonempty set of stores. Intuitively a computa- tion takes a store and returns a value together with the modified store.
%ηA(a) is (λs: S.⟨a, s⟩)
%μA(f) is (λs:S.eval(fs)), i.e. the computation that given a store s, first computes the pair computation-store ⟨f ′ , s′ ⟩ = f s and then returns
%the pair value-store ⟨a, s′′ ⟩ = f ′ s′ .


\subsection{Lawvere Theories}
Algebraic effects have a categorical basis in Lawvere Theories
This mathematical background is, regrettably, beyond the scope of this document.
In mathematical practice Lawvere theories arise whenever one has a functor into a category with finite products and one studies the natural transformations between finite products of the functor.
\cite{hyland2007category}

\section{Monads in the Wild}
Typically a programmer will think of a monad as three function signatures
which correspond to the triple $(T,\eta,\mu)$;
of course much more informally.
Firstly \texttt{fmap} corresponds to the functor $T$,
then we have \texttt{return} and \texttt{join} which correspond to
the natural transformations $\eta$ and $\mu$ respectively.
Sometimes other names are used,
for example \texttt{map} instead of \texttt{fmap} however the difference
\texttt{return}
\texttt{pure}

\begin{equation}
  \begin{split}
    fmap   &:: M x \rightarrow (x \rightarrow y) \rightarrow M y \\
    return &:: x \rightarrow M x                                 \\
    join   &:: M (M x) \rightarrow M x
  \end{split}
\end{equation}

\begin{equation}
  \begin{split}
      \lambda f.\lambda x.join\ (join\ (fmap\ f\ x))
      &\equiv
      \lambda f.\lambda x.join(fmap\ f\ (join\ x))
      \\
      \lambda f.\lambda x.join(return (fmap\ f\ x))
      &\equiv
      \lambda f.\lambda x.join(fmap\ (return \circ f)\ x))
  \end{split}
\end{equation}
Contrast these monad laws with the diagrams
\begin{center}
    \begin{tikzcd}[sep=large]
        T \rar{\eta_T} \drar[swap]{1_{C}} & T^2 \dar{\mu} & \lar[swap]{T\eta} \dlar{1_{C}} T \\
                                           & T            &
    \end{tikzcd}
    \quad
    \begin{tikzcd}[sep=large]
        T^3 \rar{T\mu} \dar[swap]{\mu_T} & T^2 \dar{\mu} \\
        T^2 \rar[swap]{\mu}                    & T
    \end{tikzcd}
\end{center}

For programmers the essence of a monad is \texttt{fmap}
this is because
however one often sees \texttt{bind} where
It is convention to use bind as the infix operator "\texttt{>>=}".
\begin{equation}
    bind :: M x \rightarrow (x \rightarrow M y) \rightarrow M y
\end{equation}

\begin{equation}
  \begin{split}
      return x >>= f      &\equiv f x \\
             m >>= return &\equiv m   \\
      (m >>= f) >>= g     &\equiv m >>= (\lambda x.(fx >>= g))
  \end{split}
\end{equation}

Kleisli composition operator

\begin{equation}
    kc   :: (x \rightarrow M y) \rightarrow (y \rightarrow M z) \rightarrow x \rightarrow M z
\end{equation}

Bind is formulated as
\begin{equation}
    g \gg f = \lambda x. join (g (f x))
\end{equation}
where we have
\begin{equation}
    (f \gg g) \gg h \equiv f \gg (g \gg h)
\end{equation}
\begin{equation}
    return \gg f \equiv f \equiv f \gg return
\end{equation}

Do programming monads need to follow the same rules as category theory monads?
There is no way for the compiler to check these rules so practically the answer is no,
however it is wise for monad implementations to obey them so that
we can chain computations in predictable and understandable ways.

\subsection{State Monad Example}

\subsection{Monads for Structuring Programs}
In this section I will illustrate how effective monads are when used to structure programs.

Firstly one should note that OCaml has non-nullable types
i.e; one will never see null where one is expecting an
int or a binary tree or anything else.
Null values are always explicit.
The canonical representation of null is the None variant.
Why is this?
Why do we need monads to this?
What do other languages do?
Monads can be used to succinctly and expressively structure computation with the option type.

\begin{verbatim}
  type 'a option = Some 'a | None
  val return : 'a -> 'a option
  val join   : 'a option option -> 'a option
  val fmap   : 'a option -> ('a -> 'b) -> 'b option
\end{verbatim}

The code for these three functions is simple and fairly self-evident.
However from this simple basis we can construct much more complicated programs which we
will be certain will never have a \textit{NullPointerException}.
Furthermore the type system will ensure 
It will refuse to compile nonsensical code which does not type-check.
Such an assurance is invaluable in creating correct programs.
Here it's important to note that the effect is the \textit{NullPointerException};
we deal with it much more cleanly and effectively 100\% of the time using monads.

\begin{verbatim}
  let return a = Some a
  let join = function
      | Some (Some a) -> Some a
      | _ -> None
  let map a f = match a with
      | None -> None
      | Some b -> f b
\end{verbatim}

Consider this example code for searching a trie data structure.
Briefly, a trie is key value data structure;
where the key is a finite sequence of values
(typically a string which is equivalent to a sequence of characters).
Each node has an option value and a list of children.
The root of the trie represents the empty string.
Two auxiliary functions \texttt{find\_child} and \texttt{val\_extract}
are used in the search code; \texttt{val\_extract}
simply returns the first value in a pair,
find child searchs the list of children returning
the node with the matching character given;
that is only should that child exists.

\begin{verbatim}
  type ('k, 'v) t = Trie of 'v option * (('k * ('k, 'v) t) list)
  val find_child  : ('k, 'v) t -> 'k -> ('k, 'v) t option
  val val_extract : ('k, 'v) t -> 'v option
  val create      : 'k list -> 'v -> ('k, 'v) t
  val get         : ('k, 'v) t -> 'k list -> 'v option

  let create key data =
    let rec aux = function
      | []      -> Trie (Some data, [])
      | c :: cs -> Trie (None, [(c, aux cs)])
    in aux key
\end{verbatim}

Create is shown to just illustrate the data structure,
we simply iterate across the list creating a node in the trie for each character
until the list is exhausted at which point we insert the value.

%TODO tikz example of the data structure

In the search code t is the trie, key is a list of characters over which
we iterate the search function. For each character we call find child on
the current node. Find child, if succesful, will return the next node
upon which we continue the search with the remaining characters. Once we
are at the last character we know to try and extract the value from the end
node. The key point here is that using bind allows us to succintly only code
for the happy case but deal with the error case at every step.

\begin{verbatim}
  let get t key =
    let rec search chars t =
        match chars with
        | []      -> val_extract t
        | c :: cs -> bind_search (find_child t c) cs
    and bind_search ot chars = ot >>= search chars
    in search key t
\end{verbatim}

The result type is similar to the option type, however we use an extra type
parameter for the unhappy case, essentially the result type encapsulates 
either an error or a correct computation result. The result monad
corresponds to a generalisation of the exception monad presented by Wadler \cite{wadler1995monads}.

\begin{verbatim}
  type ('a, 'e) result = Ok of 'a | Error of 'e
  val bind : ('a, 'e) result
          -> ('a -> ('b, 'e) result)
          -> ('b, 'e) result
\end{verbatim}

In this example we have a list assocation which is a list of pairs, in this case both
items in the pairs are strings.

\begin{verbatim}
  let list_assoc_to_job la =
    let find k = match List.Assoc.find la k with
      | None -> Error (Err.missing_key k)
      | Some v -> Ok v
    in
    find "name"   >>=                  (fun name   ->
    find "prog"   >>=                  (fun prog   ->
    find "args"   >>= parse_args   >>= (fun args   ->
    find "run_at" >>= parse_run_at >>= (fun run_at ->
      Ok (Job.create name prog args run_at ())
    ))))
\end{verbatim}

Bind is used to succintly short circuit a computation when a value can not be
correctly obtained. As such this allows the program to be structured neatly to return
an error with precise information for which key could not be found or which value could
not be parsed. The result monad is very similar to the option monad. It would be interesting
to examine whether algebraic effects can be used to structure a program in a similar manner.

Typically programming languages allow the elison of the anonymous function on the right hand side
of the bind to simply introduce the binding into the environment;
in this case OCaml ppx extension points are used in the form \textit{let\%bind}.
This is largely just a programmer convienence,
for comparison Haskell offers \textit{do notation} for the same end.

\begin{verbatim}
  let list_assoc_to_job la =
    let find k = match List.Assoc.find la k with
      | None -> Error (Err.missing_key k)
      | Some v -> Ok v
    in
    let%bind name   =  find "name"                     in
    let%bind prog   =  find "prog"                     in
    let%bind args   = (find "args"   >>= parse_args  ) in
    let%bind run_at = (find "run_at" >>= parse_run_at) in
    Ok (Job.create name prog args run_at ())
\end{verbatim}

%%TODO
\subsection{Monads for Imperative Programming}
Imperative programming is undeniably popular, and much closer to how computers physically work,
monads have been concretely shown to be an effective method of replicating
imperative style and retaining benefits of purity \cite{PeytonJones:1993}.
These examples clearly illustrate how useful monads can be for structuring a program,
however the importance and necessity of monads is that for pure functional languages
to be actually useful we require effects. Monads give us means to achieve that.

\begin{verbatim}
    result, err := myFunction()
    if err != nil {
        return nil, err
    }
\end{verbatim}

In the Go programming language one will often see this idiom,
one can easily see the relation to the result monad which we
have demonstrated.
We get this for free with monads!

\begin{verbatim}
    name, err := findName(la)
    if err != nil {
        return nil, err
    }
    prog, err := findProg(la)
    if err != nil {
        return nil, err
    }
    etc...
\end{verbatim}


\subsection{Monad Transformers}
We often find that difficulty arises when trying to compose monads,
say we have a stateful computation a



\section{Algebraic Effects}

\begin{example}
    Consider the examples of effects we gave in cite
    we will now give the operations which generate those effects
    \begin{itemize}
        \item Input/Output - print
        \item Mutable state - get/set
        \item Exceptions - raise
        \item Non-Determinism and Probabilistic Non-Determinism - choose
        \item Continuations - cont
    \end{itemize}
\end{example}
Algebraic effects, like monads, can be used to model effects in a pure language.
Where Moggi \cite{moggi1989computational} proposed monads to give categorical semantics to computational effects;
Power and Plotkin \cite{Plotkin:2002dw} propose "computational effects as being realised by
families of operations, with a monad being generated by their equational theory".
This means we can treat effects algebraically,
to actually interact with them as programming constructs algebraic effects are paired with
handlers \cite{plotkin2009handlers}.
Where algebraic operations construct effects handlers are the dual, they deconstruct effects.

Theoretical background is, regrettably, beyond the scope of this document.
\begin{definition}
    An operation is !
\end{definition}

the essence is we have operations which generate effects,
which can be dealt with algebraically %(What does algebraic mean
and further which allows generic effects
"Of the various operations, handle is of a different computational character and, although natural, it is not algebraic. Andrzej Filinski (personal communication) describes handle as a deconstructor, whereas the other operations are constructors (of effects). In this paper, we make the notion of constructor precise by identifying it with the notion of algebraic operation."

\begin{itemize}
    \item Eff: Matija Pretnar and Andrej Bauer, 2011-now.
    \item Links : Daniel Hillerström and Sam Lindley, 2015.
    \item Frank: Conor McBride, 2007, 2012.
    \item OCaml+effects: Stephen Dolan, Leo White, KC Sivaramakrishnan, 2016.
\end{itemize}

Currently algebraic effects are a popular area of research,
it has been shown that they can "concisely describe many complex control-flow constructs" \cite{leijen2017type}.
They "provide a modular abstraction for expressing effectful computation"\cite{dolan2015effective},
in this section I will examine to what extent they can be equivalently used to monads and wether
they provide the same benefits.

\begin{verbatim}
effect choice =
  | Choose : bool

effect IO =
  | Print : string -> unit
  | Read : string

effect int_state =
  | Get : int
  | Set : int -> unit

effect scheduler =
  | Spawn : (unit -> unit) -> unit
  | Yield : unit

let incr_twice () : int =
  perform (Set ((perform Get) + 1));
  perform (Set ((perform Get) + 1));
  perform (Print "incremented twice");
  perform Get
\end{verbatim}
%http://gallium.inria.fr/~scherer/doc/effect-handlers-talk.html#/sec-effect-handlers--examples

\subsection{Trie via Algebraic Effects}

\subsection{Draft}

It is important to seperate values from operations, operations give rise to effects,
there is a difference here in being and doing

"As a restriction on general monads, algebraic effects come with various advantages:
they can be freely composed, 
and there is a natural separation between their interface (as a set of operations) and their semantics (as a handler)."

%%"the signature of the effect operations forms a free algebra which gives rise to a free monad. Free monads provide a natural way to give semantics to effects, where handlers describe a fold over the algebra of operations [44]. Using a more operational perspective, we can also view algebraic effects as resumable exceptions (or perhaps as a more structured form of delimited continuations)." \cite{leijen2017type}

Monad transformers can quickly become unwieldy when there are lots of effects to manage, leading to a temptation in larger programs to combine everything into one coarse-grained state and exception monad.

[Programming and Reasoning with Algebraic Effects and Dependent Types]

%%TODO Allow us to reconcile being with doing
%%allowing the programmer to separate the expression of an effectful computation from its implementation."\cite{dolan2015effective}

Algebraic effects are computational effects that can be represented by an equational theory, or algebraic theory, whose operations produce the effects at hand.
Continuations are not algebraic effects.

Algebraic effects allow computational effects to be representable by
effects that allow a representation by opera- tions and equations

\subsection{Rebuilding our trie with algebraic effects}
\subsection{Composing algebraic effects}

\section{Abridged version}

\section{Conclusion}

\medskip

\bibliographystyle{unsrt}
\bibliography{dissert}

\end{document}
