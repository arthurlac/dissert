\section{Monads in the Wild}
Typically a programmer will think of a monad as three function signatures
which correspond to the triple $(T,\eta,\mu)$;
of course much more informally.
Firstly \texttt{fmap} corresponds to the functor $T$,
then we have \texttt{return} and \texttt{join} which correspond to
the natural transformations $\eta$ and $\mu$ respectively.
We should note that the terms \texttt{fmap,return,join}
are not fixed, relatively often other names are used;
for example \texttt{map} instead of \texttt{fmap}
or \texttt{pure/unit}instead of \texttt{return},
however the semantics are exactly the same.

\begin{equation}
  \begin{split}
    fmap   &:: M x \rightarrow (x \rightarrow y) \rightarrow M y \\
    return &:: x \rightarrow M x                                 \\
    join   &:: M (M x) \rightarrow M x
  \end{split}
\end{equation}

To be a faithful monad implementation it must be true that:
\begin{equation}
  \begin{split}
      \lambda f.\lambda x.return\ (fmap\ f\ x)
      &\equiv
      \lambda f.\lambda x.fmap\ (return \circ f)\ x)
      \\
      \lambda f.\lambda x.fmap\ f\ x
      &\equiv
      \lambda f.\lambda x.fmap\ f\ (fmap\ id\ x)
      \\
      \lambda f.\lambda x.join\ (join\ (fmap\ f\ x))
      &\equiv
      \lambda f.\lambda x.join(fmap\ f\ (join\ x))
  \end{split}
\end{equation}

Contrast these monad implementation laws with the diagrams
\begin{equation}
    \begin{tikzcd}[sep=large]
        T \rar{\eta_T} \drar[swap]{1_{C}} & T^2 \dar{\mu} & \lar[swap]{T\eta} \dlar{1_{C}} T \\
                                           & T            &
    \end{tikzcd}
    \quad
    \begin{tikzcd}[sep=large]
        T^3 \rar{T\mu} \dar[swap]{\mu_T} & T^2 \dar{\mu} \\
        T^2 \rar[swap]{\mu}                    & T
    \end{tikzcd}
\end{equation}

Do programming monads need to follow the same rules as category theory monads?
There is no way for the compiler to check these rules so practically the answer is no,
however it is wise for monad implementations to obey them so that
we can chain computations in predictable and understandable ways.

For programmers the essence of a monad is \texttt{fmap},
this is because it is our primary way to interact with the monad.
Typically we describe our desired result in terms of functions on the monad
done one after the other.
However one often sees \texttt{bind} where bind is of type
\begin{equation}
    bind :: M x \rightarrow (x \rightarrow M y) \rightarrow M y
\end{equation}
The reason why bind is as prominent,
if not more prominent, than \texttt{fmap}
is that it allows us to chain computations in a manner
which occurs more often that it does with \texttt{fmap}
%TODO
Bind is a hugely useful tool for programmers;
because often we want to sequence
Can we consider bind as a morphism between two objects in the Kleisli Category.
%TODO
\begin{equation}
    argument\
    \stackrel{f}{\rightarrow}     T\ y
    \stackrel{T g}{\rightarrow}   T\ T\ z
    \stackrel{\mu z}{\rightarrow} T\ result
\end{equation}

It is convention to use bind as the infix operator "$\bind$" where
\begin{equation}
    mx \bind f \equiv join\ (fmap\ f\ mx)
\end{equation}
Consider that \texttt{join} can be expressed in terms of \texttt{bind} (with \texttt{id})
\begin{equation}
    join\ m \equiv m \bind id
\end{equation}
and furthermore so can \texttt{fmap} (with \texttt{return}).
\begin{equation}
    fmap\ f\ x \equiv (return\ x) \bind f
\end{equation}

Because \texttt{bind} is often found with \texttt{(fmap,join,return)}
We can reformulate our axioms in terms of bind,
indeed the axioms are often reformulated this way because it is simpler.
\begin{equation}
  \begin{split}
        return\ x \bind f &\equiv f x \\
           m \bind return &\equiv m   \\
      (m \bind f) \bind g &\equiv m \bind (\lambda x.(fx \bind g))
  \end{split}
\end{equation}

We can further rephrase ourselves in terms
of the \textit{Kleisli composition operator} $\gg$ where
\begin{equation}
    \gg :: (x \rightarrow M y) \rightarrow (y \rightarrow M z) \rightarrow (x \rightarrow M z)
\end{equation}
and we have
\begin{equation}
    (f \gg g) \gg h \equiv f \gg (g \gg h)
\end{equation}
\begin{equation}
    return \gg f \equiv f \equiv f \gg return
\end{equation}

\subsection{State Monad Example}
In this section we will illustrate how closely 
the $\lambda_c$-calculus relates to actual code,
similar to what is in the Haskell standard library.
Furthermore we will show that from the relatively
simple triple $(fmap/bind,return,join)$ we can

Firstly recall our example \ref{lc1}
computations with side-effects:
\begin{itemize}
    \item $T(-)$ is the functor $(-\times S)^S$, where $S$ is a nonempty set of stores.
        Intuitively a computation takes a store and returns a value together with the modified store.
    \item $\eta_A$ is the map $a \rightarrow (\lambda s.\langle a,s \rangle)$
    \item $\mu_A$ is the map $f \rightarrow (\lambda s.eval(fs))$,
        i.e. $\mu_A(f)$ is the computation that given a store $s$,
        first computes the pair computation-store $\langle f\prime,s\prime\rangle = fs$
        and then returns the pair value-store $\langle a,s\prime\prime\rangle = f\prime s\prime$.
\end{itemize}

This translates to the code
this code is from the Haskell standard library

Firstly consider
$\eta_A$ the map $a \mapsto (\lambda s.\langle a,s \rangle)$
$$\eta\ x = State\ (\lambda s.(x,s))$$
\begin{verbatim}
    return x = State (\ s -> (x,s))
\end{verbatim}

$\mu_A$ is the map $f \rightarrow (\lambda s.eval(fs))$,
i.e. $\mu_A(f)$ is the computation that given a store $s$,
first computes the pair computation-store $\langle f\prime,s\prime\rangle = fs$
and then returns the pair value-store $\langle a,s\prime\prime\rangle = f\prime s\prime$.
$$\mu\ mmx = State (\lambda s.\ uncurry\ runState\ (runState mmx s))$$
where \texttt{uncurry :: (a -> b -> c) -> ((a, b) -> c)}

Next fmap

\begin{verbatim}
    mx >>= \ s -> \f -> f s
    evalState (sequence $ repeat $ do { n <- get; put (n*2); return n }) 1
\end{verbatim}

\begin{verbatim}
newtype State s a = State { runState :: s -> (a,s) }  
\end{verbatim}

However to meaningfully have state we actually need
get and put
and some way to run the state
$$(\$) :: (a -> b) -> a -> b$$

\begin{verbatim}
newtype State s a = State {runState :: s -> (a, s)}
instance Monad (State s) where
    return x = State $ \s -> (x,s)

    (State h) >>= f = State $ \s -> let (a, newState) = h s
                                        (State g) = f a
                                    in  g newState
                                    instance Functor (State s) where


    fmap f m = State $ \s -> case runState m s of
                                 (a, s') -> (f a, s')

join :: (State s (State s a)) -> (State s a)
join xss = State (\s -> uncurry runState (runState xss s))

instance Monad (State s) where
    return x = State (\ s -> (x,s))
    m >>= f  = State (\ s ->
        let (x,s') = runState m s in
        runState (f x) s')

-- | Return the state from the internals of the monad.
get :: m s
get = state (\s -> (s, s))

-- | Replace the state inside the monad.
put :: s -> m ()
put s = state (\_ -> ((), s))

-- | Embed a simple state action into the monad.
state :: (s -> (a, s)) -> m a
state f = do
  s <- get
  let ~(a, s') = f s
  put s'
  return a

import Control.Monad.State

pop :: State Stack Int
pop = State $ \(x:xs) -> (x,xs)

push :: Int -> State Stack ()
push a = State $ \xs -> ((),a:xs)
import System.Random
import Control.Monad.State

randomSt :: (RandomGen g, Random a) => State g a
randomSt = State random
\end{verbatim}

\subsection{Monads for Structuring Programs}
In this section I will illustrate how effective monads are when used to structure programs.
Firstly one should note that OCaml has non-nullable types
i.e; one will never see null or nil where one is expecting an
int or a binary tree or anything else.
Null values are always explicit.
The canonical representation of null is the None variant.
Monads can be used to succinctly and expressively structure computation with the option type.
%TODO
%Why is this?
%Why do we need monads to this?
%What do other languages do?

\begin{verbatim}
  type 'a option = Some 'a | None
  val return : 'a -> 'a option
  val join   : 'a option option -> 'a option
  val fmap   : 'a option -> ('a -> 'b) -> 'b option
\end{verbatim}

The code for these three functions is simple and fairly self-evident.
However from this simple basis we can construct much more complicated programs which we
will be certain will never have a \textit{NullPointerException}.
Furthermore the type system will ensure 
It will refuse to compile nonsensical code which does not type-check.
Such an assurance is invaluable in creating correct programs.
Here it's important to note that the effect is the \textit{NullPointerException};
we deal with it much more cleanly and effectively 100\% of the time using monads.

\begin{verbatim}
  let return a = Some a
  let join = function
      | Some (Some a) -> Some a
      | _ -> None
  let map a f = match a with
      | None -> None
      | Some b -> f b
\end{verbatim}

Consider this example code for searching a trie data structure.
Briefly, a trie is key value data structure;
where the key is a finite sequence of values
(typically a string which is equivalent to a sequence of characters).
Each node has an option value and a list of children.
The root of the trie represents the empty string.
Two auxiliary functions \texttt{find\_child} and \texttt{val\_extract}
are used in the search code; \texttt{val\_extract}
simply returns the first value in a pair,
find child searchs the list of children returning
the node with the matching character given;
that is only should that child exists.

\begin{verbatim}
  type ('k, 'v) t = Trie of 'v option * (('k * ('k, 'v) t) list)
  val find_child  : ('k, 'v) t -> 'k -> ('k, 'v) t option
  val val_extract : ('k, 'v) t -> 'v option
  val create      : 'k list -> 'v -> ('k, 'v) t
  val get         : ('k, 'v) t -> 'k list -> 'v option

  let create key data =
    let rec aux = function
      | []      -> Trie (Some data, [])
      | c :: cs -> Trie (None, [(c, aux cs)])
    in aux key
\end{verbatim}

Create is shown to just illustrate the data structure,
we simply iterate across the list creating a node in the trie for each character
until the list is exhausted at which point we insert the value.

%TODO tikz example of the data structure

In the search code t is the trie, key is a list of characters over which
we iterate the search function. For each character we call find child on
the current node. Find child, if succesful, will return the next node
upon which we continue the search with the remaining characters. Once we
are at the last character we know to try and extract the value from the end
node. The key point here is that using bind allows us to succintly only code
for the happy case but deal with the error case at every step.

\begin{verbatim}
  let get t key =
    let rec search chars t =
        match chars with
        | []      -> val_extract t
        | c :: cs -> bind_search (find_child t c) cs
    and bind_search ot chars = ot >>= search chars
    in search key t
\end{verbatim}

The result type is similar to the option type, however we use an extra type
parameter for the unhappy case, essentially the result type encapsulates 
either an error or a correct computation result. The result monad
corresponds to a generalisation of the exception monad presented by Wadler \cite{wadler1995monads}.

\begin{verbatim}
  type ('a, 'e) result = Ok of 'a | Error of 'e
  val bind : ('a, 'e) result
          -> ('a -> ('b, 'e) result)
          -> ('b, 'e) result
\end{verbatim}

In this example we have a list assocation which is a list of pairs, in this case both
items in the pairs are strings.

\begin{verbatim}
  let list_assoc_to_job la =
    let find k = match List.Assoc.find la k with
      | None -> Error (Err.missing_key k)
      | Some v -> Ok v
    in
    find "name"   >>=                  (fun name   ->
    find "prog"   >>=                  (fun prog   ->
    find "args"   >>= parse_args   >>= (fun args   ->
    find "run_at" >>= parse_run_at >>= (fun run_at ->
      Ok (Job.create name prog args run_at ())
    ))))
\end{verbatim}

Bind is used to succintly short circuit a computation when a value can not be
correctly obtained. As such this allows the program to be structured neatly to return
an error with precise information for which key could not be found or which value could
not be parsed. The result monad is very similar to the option monad. It would be interesting
to examine whether algebraic effects can be used to structure a program in a similar manner.

Typically programming languages allow the elison of the anonymous function on the right hand side
of the bind to simply introduce the binding into the environment;
in this case OCaml ppx extension points are used in the form \textit{let\%bind}.
This is largely just a programmer convienence,
for comparison Haskell offers \textit{do notation} for the same end.

\begin{verbatim}
  let list_assoc_to_job la =
    let find k = match List.Assoc.find la k with
      | None -> Error (Err.missing_key k)
      | Some v -> Ok v
    in
    let%bind name   =  find "name"                     in
    let%bind prog   =  find "prog"                     in
    let%bind args   = (find "args"   >>= parse_args  ) in
    let%bind run_at = (find "run_at" >>= parse_run_at) in
    Ok (Job.create name prog args run_at ())
\end{verbatim}
Is equivalent to
\begin{verbatim}
  let list_assoc_to_job la =
    let find k = match List.Assoc.find la k with
      | None -> Error (Err.missing_key k)
      | Some v -> Ok v
    in
    find "name"   >>=                  (fun name   ->
    find "prog"   >>=                  (fun prog   ->
    find "args"   >>= parse_args   >>= (fun args   ->
    find "run_at" >>= parse_run_at >>= (fun run_at ->
      Ok (Job.create name prog args run_at ())
    ))))
\end{verbatim}

\texttt{do} is simply syntactic sugar for \texttt{>>=} and \texttt{>>} where
\begin{equation}
    \texttt{>> :: M x -> M y -> M y}
\end{equation}
i.e. do first and ignore the result and then do the second.
\begin{align}
    \texttt{do}\ \{ x;\ \texttt{<stmts>} \}
    &\equiv x \texttt{ >> do \{<stmts>\}}
    \\
    \texttt{do}\ \{ v \leftarrow x\ \texttt{ <stmts>}\}
    &\equiv x \texttt{ >>= } \lambda v.\ \texttt{do}\ \{ \texttt{<stmts>} \}
    \\
    \texttt{do}\ \{\texttt{let }x = v\ \texttt{<stmts>}\}
    &\equiv (\lambda x.\ \texttt{do}\ \{ \texttt{<stmts>} \})v
\end{align}

In this example we simply and effectively deal with two types of errors.
Firstly a key being missing from our list
\begin{verbatim}
    let find k = match List.Assoc.find la k with
      | None -> Error (Err.missing_key k)
      | Some v -> Ok v
\end{verbatim}
Secondly a more complicated value which we have to parse;
the parsing of which can fail
\begin{verbatim}
    let%bind run_at = (find "run_at" >>= parse_run_at) in
\end{verbatim}
The key point here is when we have an error we want to fail
loudly and give as much information as possible about why we failed.
Furthermore, we don't want to take care of that in this function
we don't want to put that logic here
Monads allow us to sequence this error handling
and keep the necessary logic in a sensible re-usable place.

Finally consider the relation between
\texttt{(let\%bind,do)} and the Kleisli category of programs.
Consider how convienent it is for programmers to have a category
of programs where reliable error handling is built in by default.

%%TODO
\subsection{Monads for Imperative Programming}
Imperative programming is undeniably popular,
and much closer to how computers physically work than functional programming;
There is a bridge to be crossed between functional and imperative programming

monads have been concretely shown to be an effective method of replicating
imperative style and retaining benefits of purity \cite{PeytonJones:1993}.
These examples clearly illustrate how useful monads can be for structuring a program,
however the importance and necessity of monads is that for pure functional languages
to be actually useful we require effects. Monads give us means to achieve that.

In this section we will prove the inverse,
monads are good for imperative programming
and imperative programmers have monad-like features



"Moreover, effect handlers allow con- current programs to be written in direct-style retaining the simplicity of sequential code as opposed to callback-oriented style with either monadic concurrency libraries such as Lwt [8] and Async [5] for OCaml or explicit callbacks."
\cite{dolaneffectively}

"The use of monads to structure functional programs is de- scribed. Monads provide a convenient framework for simulating effects found in other languages, such as global state, exception handling, out- put, or non-determinism. Three case studies are looked at in detail: how monads ease the modification of a simple evaluator; how monads act as the basis of a datatype of arrays subject to in-place update; and how monads can be used to build parsers.
"
"Pure functional languages have this advantage: all flow of data is made explicit. And this disadvantage: sometimes it is painfully explicit."
"It is with regard to modularity that explicit data flow becomes both a blessing and a curse. On the one hand, it is the ultimate in modularity. All data in and all data out are rendered manifest and accessible, providing a maximum of flexibility. On the other hand, it is the nadir of modularity. The essence of an algorithm can become buried under the plumbing required to carry data from its point of creation to its point of use"
\cite{wadler1995monads}

"
%Do es the monadic style force one􏰄 in e􏰒ect􏰄 to write a functional facsimile of an
%imp erative program, thereby losing any advantages of writing in a functional language?
%the p ower of higher􏰆order functions and non􏰆strict semantics can b e used
%programming easier􏰄 by de􏰐ning new action􏰆manipulating combinators􏰇
%The p oint we are making is that it is easy for the programmer to de􏰐ne new 􏰜glue􏰝 to combine actions in just the way which is suitable for the program b eing written􏰇 It􏰚s a bit like b eing able to de􏰐ne your own control structures in an imp erative language􏰇
"\cite{PeytonJones:1993}


"The special characteristics and advantages of functional programming are often summed up more or less as follows. Functional programs contain no assignment statements, so variables, once given a value, never change. More generally, functional programs contain no side-effects at all. A function call can have no effect other than to compute its result. This eliminates a major source of bugs, and also makes the order of execution irrelevant - since no side-effect can change the value of an expression, it can be evaluated at any time. This relieves the programmer of the burden of prescribing the flow of control."
"When writing a modular program to solve a problem, one first divides the problem into sub- problems, then solves the sub-problems and combines the solutions. The ways in which one can divide up the original problem depend directly on the ways in which one can glue solutions together. Therefore, to increase ones ability to modularise a problem conceptually, one must provide new kinds of glue in the programming language."
\cite{hughes1989functional}

In the Go programming language one will often see this idiom,
one can easily see the relation to the result monad which we
have demonstrated.
We get this for free with monads!
Go does not have exceptions,

\begin{verbatim}
    result, err := myFunction()
    if err != nil {
        return nil, err
    }
\end{verbatim}

Consider our error handling example;
in Go this might be much more verbose
and perhaps painfully explicit
Of course one would probably structure the program differently in Go;
however the OCaml implementation is undeniably straightforward
and error resistant, while providing useful error messages.

\begin{verbatim}
    name, err := findName(la)
    if err != nil {
        return nil, err
    }
    prog, err := findProg(la)
    if err != nil {
        return nil, err
    }
    etc...
    job.Create(name, prog, args, run_at)
\end{verbatim}

\subsection{Monad Transformers}
The elephant in the room

Now we must discuss how one composes monads;
say

Monad transformers add functionality of one monad to another.
We often find that difficulty arises when trying to compose monads,
say we have a stateful computation a

"Monad transformers add functionality of one monad to another. They do so by stacking a transformer version of the additional monad on top of the original one, which with enough repetition results in something like a monadic Voltron."
"Formally, monad transformers are illustrated well by the lift function: the only function in the MonadTrans class. For regular monads its type signature looks like this:

\begin{verbatim}
liftM :: Monad m => (a -> r) -> m a -> m r
whereas for monad transformers its type signature looks like this:

lift :: Monad m => m a -> t m a
where m is any monad and t is the transformer."
\end{verbatim}



"Monad transformers have an inherent limitation: they enforce the static ordering of effect layers and hence statically fixed effect interactions. There are practically significant computations that require interleaving of effects. ‘Delimited Dynamic Binding’ (ICFP 2006) was first to bring up this point. The ‘Extensible Effects’ paper expanded that discussion on new examples. Section 5 describes simple and common programming patterns that are particularly problematic with monad transformers because the static ordering of effect layers is not flexible.

"

"A monad transformer is similar to a regular monad, but it's not a standalone entity: instead, it modifies the behaviour of an underlying monad. Most of the monads in the mtl library have transformer equivalents. By convention, the transformer version of a monad has the same name, with a T stuck on the end. For example, the transformer equivalent of State is StateT; it adds mutable state to an underlying monad. The WriterT monad transformer makes it possible to write data when stacked on top of another monad."

"As we have already mentioned, when we stack a monad transformer on a normal monad, the result is another monad. This suggests the possibility that we can again stack a monad transformer on top of our combined monad, to give a new monad, and in fact this is a common thing to do. Under what circumstances might we want to create such a stack? 1 comment

If we need to talk to the outside world, we'll have IO at the base of the stack. Otherwise, we will have some normal monad. 4 comments

If we add a ReaderT layer, we give ourselves access to read-only configuration information. No comments

Add a StateT layer, and we gain global state that we can modify. 2 comments

Should we need the ability to log events, we can add a WriterT layer. No comments

The power of this approach is that we can customise the stack to our exact needs, specifying which kinds of effects we want to support. No comments

As a small example of stacked monad transformers in action, here is a reworking of the countEntries function we developed earlier. We will modify it to recurse no deeper into a directory tree than a given amount, and to record the maximum depth it reaches.

In the framework that mtl provides, each monad transformer in the stack makes the API of a lower level available by providing instances of a host of typeclasses. We could follow this pattern, and write a number of boilerplate instances.


In this case, the only way we can access the underlying State monad's put is through use of lift. No comments

\begin{verbatim}
-- file: ch18/StackStack.hs
innerPut :: String -> Foo ()
innerPut = lift . put
\end{verbatim}

Stacking monad transformers is analogous to composing functions. If we change the order in which we apply functions, and we then get different results, we are not surprised. So it is with monad transformers, too.

Finally, when we use monads and monad transformers, we can pay an efficiency tax. For instance, the State monad carries its state around in a closure. Closures might be cheap in a Haskell implementation, but they're not free.
"
\cite{o2008real}









Note that OCaml is impure and doesn't have monad transformers,
it doesn't need them.
Why?
How does this interact with modules composing?



\cite{king1993combining}
%describe how some monads may b e combined with others to yield a combined monad􏰀
%For our purp oses􏰂 we will think of a monad as a typ e constructor􏰂 together with three functions that must satisfy certain laws􏰀 For instance􏰂 we may have an interpreter and wish it to return􏰂 not just a value􏰂 but the numb er of reduc􏰉 tion steps taken to reach the value􏰀 Later􏰂 we may want our interpreter either to return a value or an error message􏰀 Without using a monad or a similar discipline􏰂 it would be a messy undertaking to do in a purely functional pro􏰉 gramming language􏰀 If however our interpreter was written in a monadic style􏰂 we would merely have to change the monad to change what extra information the interpreter should return􏰀


%are now two distinct approaches to combining E x with S t􏰂 either􏰊
%type S tE x a 􏰏 S tate 􏰝 􏰌E x a􏰚 S tate􏰎
%type E xS t a 􏰏 S tate 􏰝 E x 􏰌a􏰚 S tate􏰎
%these have entirely di􏰓erent meanings􏰀 In the 􏰁rst mo del􏰂 an expression that raises an exception also returns a state􏰀 This monad may b e thought of as mo delling the language Standard ML􏰂 where when an exception is raised the state survives􏰀 In the second mo del􏰂 if an expression raises an exception then
%the
%state is not returned􏰀

"
A fundamental problem with monad transformer stacks is that
once a particular abstract effect is instantiated,
the order of effects in the stack becomes concrete,
and it becomes necessary to explicitly lift operations
through the stack."
\cite{kammar2013handlers}
